{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date      Time  Latitude  Longitude Speed TrackAngle\n",
      "0      09/05/2023  22:01:00   53.9432    10.8564  0.16          0\n",
      "1      09/05/2023  22:01:01   53.9432    10.8564  0.16          0\n",
      "2      09/05/2023  22:01:03   53.9432    10.8564   0.1          0\n",
      "3      09/05/2023  22:01:04   53.9432    10.8564  0.14          0\n",
      "4      09/05/2023  22:01:05   53.9432    10.8564  0.14          0\n",
      "...           ...       ...       ...        ...   ...        ...\n",
      "68912  10/05/2023  21:59:54   53.8353    10.5099   0.6     203.21\n",
      "68913  10/05/2023  21:59:55   53.8353    10.5099  0.22     203.21\n",
      "68914  10/05/2023  21:59:56   53.8353    10.5099  0.31     203.21\n",
      "68915  10/05/2023  21:59:57   53.8353    10.5099  0.38     203.21\n",
      "68916  10/05/2023  21:59:58   53.8353    10.5099  0.45     203.21\n",
      "\n",
      "[68917 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load GPS data with path\n",
    "with open('./Data/2/GPS/gps_2023-05-10.log', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Filtering out clean data \n",
    "cleaned_data = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        # Split the line by semicolon ';'\n",
    "        data_elements = line.split(';')\n",
    "        # Check if the line has exactly 7 elements and ends with a '/'\n",
    "        if len(data_elements) == 7 and data_elements[-1].strip() == '/':\n",
    "            # Append to the cleaned_data\n",
    "            cleaned_data.append(data_elements[:-1])  # Discard the last element (slash)\n",
    "\n",
    "# Create a DataFrame from the cleaned data\n",
    "column_names = ['Date', 'Time', 'Latitude', 'Longitude', 'Speed', 'TrackAngle']\n",
    "data_gps2 = pd.DataFrame(cleaned_data, columns=column_names)\n",
    "\n",
    "# Data cleaning and formatting\n",
    "data_gps2['Latitude'] = data_gps2['Latitude'].str.rstrip('N').astype(float)\n",
    "data_gps2['Longitude'] = data_gps2['Longitude'].str.rstrip('E').astype(float)\n",
    "\n",
    "print(data_gps2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UnixTimeStampInMsec            Datetime  Latitude  Longitude Speed  \\\n",
      "0            1683669660000 2023-05-09 22:01:00   53.9432    10.8564  0.16   \n",
      "1            1683669661000 2023-05-09 22:01:01   53.9432    10.8564  0.16   \n",
      "2            1683669663000 2023-05-09 22:01:03   53.9432    10.8564   0.1   \n",
      "3            1683669664000 2023-05-09 22:01:04   53.9432    10.8564  0.14   \n",
      "4            1683669665000 2023-05-09 22:01:05   53.9432    10.8564  0.14   \n",
      "...                    ...                 ...       ...        ...   ...   \n",
      "68912        1683755994000 2023-05-10 21:59:54   53.8353    10.5099   0.6   \n",
      "68913        1683755995000 2023-05-10 21:59:55   53.8353    10.5099  0.22   \n",
      "68914        1683755996000 2023-05-10 21:59:56   53.8353    10.5099  0.31   \n",
      "68915        1683755997000 2023-05-10 21:59:57   53.8353    10.5099  0.38   \n",
      "68916        1683755998000 2023-05-10 21:59:58   53.8353    10.5099  0.45   \n",
      "\n",
      "      TrackAngle  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "...          ...  \n",
      "68912     203.21  \n",
      "68913     203.21  \n",
      "68914     203.21  \n",
      "68915     203.21  \n",
      "68916     203.21  \n",
      "\n",
      "[68917 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge 'Date' and 'Time' columns into a single datetime column\n",
    "data_gps2['Datetime'] = pd.to_datetime(data_gps2['Date'] + ' ' + data_gps2['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Convert the datetime column to Unix format in milliseconds\n",
    "data_gps2['UnixTimeStampInMsec'] = (data_gps2['Datetime'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(milliseconds=1)\n",
    "\n",
    "# Reorder the columns, moving 'UnixTimeInMSec' to the first position\n",
    "data_gps2 = data_gps2[['UnixTimeStampInMsec', 'Datetime', 'Latitude', 'Longitude', 'Speed', 'TrackAngle']]\n",
    "\n",
    "# Print the DataFrame with Unix timestamps in the first column\n",
    "print(data_gps2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "       UnixTimeStampInMsec                       Datetime      Latitude  \\\n",
      "count         6.891700e+04                          68917  68917.000000   \n",
      "mean          1.683713e+12  2023-05-10 10:03:42.738279424     53.876192   \n",
      "min           1.683670e+12            2023-05-09 22:01:00     53.820400   \n",
      "25%           1.683691e+12            2023-05-10 03:59:56     53.835100   \n",
      "50%           1.683714e+12            2023-05-10 10:18:47     53.835300   \n",
      "75%           1.683731e+12            2023-05-10 15:08:50     53.943200   \n",
      "max           1.683756e+12            2023-05-10 21:59:58     53.943400   \n",
      "std           2.444956e+07                            NaN      0.052246   \n",
      "\n",
      "          Longitude  \n",
      "count  68917.000000  \n",
      "mean      10.642034  \n",
      "min       10.494500  \n",
      "25%       10.509800  \n",
      "50%       10.509900  \n",
      "75%       10.856400  \n",
      "max       10.859000  \n",
      "std        0.167193  \n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(data_gps2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as GPSSet1Data.csv in the CleanData/Set2/ Folder.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "data_gps2.to_csv('./CleanData/Set2/GPSSet2Data.csv', index=False)\n",
    "\n",
    "print(\"DataFrame saved as GPSSet1Data.csv in the CleanData/Set2/ Folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Analyze second set Shock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1 has non-numeric data. Discarding the row.\n",
      "Row 1994 does not have the correct number of columns. Discarding the row.\n",
      "Row 8850 does not have the correct number of columns. Discarding the row.\n",
      "Row 9936 does not have the correct number of columns. Discarding the row.\n",
      "Row 24329 does not have the correct number of columns. Discarding the row.\n",
      "Row 346150 does not have the correct number of columns. Discarding the row.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_data.txt' with the path to your data file\n",
    "with open('./Data/2/shock/shock_2023-05-10.log', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Clean and process the lines\n",
    "cleaned_lines = []\n",
    "for line_idx, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        # Split the line by semicolon ';'\n",
    "        data_elements = line.split(';')\n",
    "        # Check if the line has 8 elements (7 data columns + 1 timestamp)\n",
    "        if len(data_elements) == 8:\n",
    "            # Check if all elements in the row are numeric\n",
    "            if all(pd.to_numeric(pd.Series(data_elements), errors='coerce').notnull()):\n",
    "                # If all elements are numeric, keep the row\n",
    "                cleaned_lines.append(data_elements)\n",
    "            else:\n",
    "                print(f\"Row {line_idx+1} has non-numeric data. Discarding the row.\")\n",
    "        else:\n",
    "            print(f\"Row {line_idx+1} does not have the correct number of columns. Discarding the row.\")\n",
    "\n",
    "# Check if the first column name is 'Timestamp'\n",
    "if cleaned_lines[0][0] != 'Timestamp':\n",
    "    # Change the column names accordingly\n",
    "    column_names = ['Timestamp', 'AccelX', 'AccelY', 'AccelZ', 'GyroX', 'GyroY', 'GyroZ', 'Temperature']\n",
    "else:\n",
    "    column_names = cleaned_lines[0]\n",
    "\n",
    "# Create a DataFrame from the cleaned data\n",
    "data_shock2 = pd.DataFrame(cleaned_lines[1:], columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UnixTimeStampInMsec     AccelX     AccelY    AccelZ     GyroX  \\\n",
      "0            1683669630663   0.002776   0.000516  9.923157  0.009163   \n",
      "1            1683669630746   0.000230   0.000377  9.919678  0.008858   \n",
      "2            1683669630830  -0.000812   0.002910  9.923219  0.009163   \n",
      "3            1683669630913   0.002775   0.001711  9.923110  0.009163   \n",
      "4            1683669630996   0.003819  -0.003213  9.919663  0.009163   \n",
      "...                    ...        ...        ...       ...       ...   \n",
      "853042       1683755999627   0.005431  -0.008674  9.928580  0.009163   \n",
      "853043       1683755999711   0.002988  -0.004009  9.926007  0.009468   \n",
      "853044       1683755999795   0.002942  -0.008904  9.923849  0.009163   \n",
      "853045       1683755999879   0.000597  -0.006398  9.926171  0.008858   \n",
      "853046       1683755999963   0.001769  -0.007651  9.925010  0.009163   \n",
      "\n",
      "            GyroY      GyroZ Temperature  \n",
      "0       -0.003360  -0.009468   36.000000  \n",
      "1       -0.003971  -0.009468   36.000000  \n",
      "2       -0.003665  -0.009468   36.000000  \n",
      "3       -0.003665  -0.009774   36.000000  \n",
      "4       -0.003665  -0.009774   36.000000  \n",
      "...           ...        ...         ...  \n",
      "853042  -0.003971  -0.009774   35.000000  \n",
      "853043  -0.003971  -0.009774   35.000000  \n",
      "853044  -0.004276  -0.009774   35.000000  \n",
      "853045  -0.003971  -0.009774   35.000000  \n",
      "853046  -0.003971  -0.009774   35.000000  \n",
      "\n",
      "[853047 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Renaming Timestamp column name \n",
    "data_shock2.rename(columns={'Timestamp': 'UnixTimeStampInMsec'}, inplace=True)\n",
    "print(data_shock2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as ShockSet1Data.csv in the CleanData/Set2 Folder.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "data_shock2.to_csv('./CleanData/Set2/ShockSet1Data.csv', index=False)\n",
    "\n",
    "print(\"DataFrame saved as ShockSet1Data.csv in the CleanData/Set2 Folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging GPS Data and Shock Data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UnixTimeStampInMsec            Datetime  Latitude  Longitude Speed  \\\n",
      "0            1683669660000 2023-05-09 22:01:00   53.9432    10.8564  0.16   \n",
      "1            1683669661000 2023-05-09 22:01:01   53.9432    10.8564  0.16   \n",
      "2            1683669663000 2023-05-09 22:01:03   53.9432    10.8564   0.1   \n",
      "3            1683669664000 2023-05-09 22:01:04   53.9432    10.8564  0.14   \n",
      "4            1683669665000 2023-05-09 22:01:05   53.9432    10.8564  0.14   \n",
      "...                    ...                 ...       ...        ...   ...   \n",
      "68912        1683755994000 2023-05-10 21:59:54   53.8353    10.5099   0.6   \n",
      "68913        1683755995000 2023-05-10 21:59:55   53.8353    10.5099  0.22   \n",
      "68914        1683755996000 2023-05-10 21:59:56   53.8353    10.5099  0.31   \n",
      "68915        1683755997000 2023-05-10 21:59:57   53.8353    10.5099  0.38   \n",
      "68916        1683755998000 2023-05-10 21:59:58   53.8353    10.5099  0.45   \n",
      "\n",
      "      TrackAngle  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "...          ...  \n",
      "68912     203.21  \n",
      "68913     203.21  \n",
      "68914     203.21  \n",
      "68915     203.21  \n",
      "68916     203.21  \n",
      "\n",
      "[68917 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_gps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UnixTimeStampInMsec     AccelX     AccelY    AccelZ     GyroX  \\\n",
      "0            1683669630663   0.002776   0.000516  9.923157  0.009163   \n",
      "1            1683669630746   0.000230   0.000377  9.919678  0.008858   \n",
      "2            1683669630830  -0.000812   0.002910  9.923219  0.009163   \n",
      "3            1683669630913   0.002775   0.001711  9.923110  0.009163   \n",
      "4            1683669630996   0.003819  -0.003213  9.919663  0.009163   \n",
      "...                    ...        ...        ...       ...       ...   \n",
      "853042       1683755999627   0.005431  -0.008674  9.928580  0.009163   \n",
      "853043       1683755999711   0.002988  -0.004009  9.926007  0.009468   \n",
      "853044       1683755999795   0.002942  -0.008904  9.923849  0.009163   \n",
      "853045       1683755999879   0.000597  -0.006398  9.926171  0.008858   \n",
      "853046       1683755999963   0.001769  -0.007651  9.925010  0.009163   \n",
      "\n",
      "            GyroY      GyroZ Temperature  \n",
      "0       -0.003360  -0.009468   36.000000  \n",
      "1       -0.003971  -0.009468   36.000000  \n",
      "2       -0.003665  -0.009468   36.000000  \n",
      "3       -0.003665  -0.009774   36.000000  \n",
      "4       -0.003665  -0.009774   36.000000  \n",
      "...           ...        ...         ...  \n",
      "853042  -0.003971  -0.009774   35.000000  \n",
      "853043  -0.003971  -0.009774   35.000000  \n",
      "853044  -0.004276  -0.009774   35.000000  \n",
      "853045  -0.003971  -0.009774   35.000000  \n",
      "853046  -0.003971  -0.009774   35.000000  \n",
      "\n",
      "[853047 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_shock2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# Check datatype of common column for merging\n",
    "print(data_gps2['UnixTimeStampInMsec'].dtype)\n",
    "print(data_shock2['UnixTimeStampInMsec'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Merge the two DataFrames based on the 'UnixTimeStampInMsec' column\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m combined_data_set2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(data_gps2, data_shock2, on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mUnixTimeStampInMsec\u001b[39;49m\u001b[39m'\u001b[39;49m, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minner\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(combined_data_set2)\n",
      "File \u001b[1;32md:\\MyWorkspace\\ShockSensorAnalysis\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:148\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 148\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    149\u001b[0m         left,\n\u001b[0;32m    150\u001b[0m         right,\n\u001b[0;32m    151\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    152\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    153\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    154\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    155\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    156\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    157\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    158\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    159\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    160\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32md:\\MyWorkspace\\ShockSensorAnalysis\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:741\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    733\u001b[0m (\n\u001b[0;32m    734\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    735\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    736\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m    737\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merge_keys()\n\u001b[0;32m    739\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 741\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_coerce_merge_keys()\n\u001b[0;32m    743\u001b[0m \u001b[39m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[39m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[39m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MyWorkspace\\ShockSensorAnalysis\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1401\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1395\u001b[0m     \u001b[39m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m     \u001b[39melif\u001b[39;00m (\n\u001b[0;32m   1397\u001b[0m         inferred_left \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_right \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[0;32m   1398\u001b[0m     ) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   1399\u001b[0m         inferred_right \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_left \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[0;32m   1400\u001b[0m     ):\n\u001b[1;32m-> 1401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1403\u001b[0m \u001b[39m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m \u001b[39melif\u001b[39;00m needs_i8_conversion(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_i8_conversion(rk\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames based on the 'UnixTimeStampInMsec' column\n",
    "combined_data_set2 = pd.merge(data_gps2, data_shock2, on='UnixTimeStampInMsec', how='inner')\n",
    "print(combined_data_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Visualization\n",
    "# Plot time series data of shock and GPS parameters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_data_set2['UnixTimeStampInMsec'], combined_data_set2['AccelX'], label='AccelX')\n",
    "plt.plot(combined_data_set2['UnixTimeStampInMsec'], combined_data_set2['AccelY'], label='AccelY')\n",
    "plt.plot(combined_data_set2['UnixTimeStampInMsec'], combined_data_set2['AccelZ'], label='AccelZ')\n",
    "plt.xlabel('UnixTimeStampInMsec')\n",
    "plt.ylabel('Acceleration')\n",
    "plt.title('Acceleration Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot speed and track angle\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_data_set2['UnixTimeStampInMsec'], combined_data_set2['Speed'], label='Speed')\n",
    "plt.plot(combined_data_set2['UnixTimeStampInMsec'], combined_data_set2['TrackAngle'], label='TrackAngle')\n",
    "plt.xlabel('UnixTimeStampInMsec')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Speed and Track Angle Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering on data (Latitude and Longitude)\n",
    "clustered_data = combined_data_set2[['Latitude', 'Longitude']]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "merged_gps_data_scaled = scaler.fit_transform(clustered_data)\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow method\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(merged_gps_data_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow method\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Based on the Elbow method, choose an appropriate number of clusters and fit the K-means model\n",
    "num_clusters = 3  # You can adjust this based on the Elbow plot\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "combined_data_set2['Cluster'] = kmeans.fit_predict(merged_gps_data_scaled)\n",
    "\n",
    "# Visualize the clusters on a map\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Longitude', y='Latitude', hue='Cluster', data=combined_data_set2, palette='viridis', s=50)\n",
    "plt.title('Clustering of GPS Coordinates')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
